{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and define tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "src_file = \"data/formatted_data.csv\"\n",
    "dst_dir = \"processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_existing_length(data):\n",
    "    res = 0\n",
    "    for i in data:\n",
    "        if not pd.isna(i):\n",
    "            res += 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def fill_missing_value(data, to_fill_value=0):\n",
    "    data_len = len(data)\n",
    "    data_exist_len = calculate_data_existing_length(data)\n",
    "    if data_len == data_exist_len:\n",
    "        return data\n",
    "    elif data_exist_len == 0:\n",
    "        # data = [to_fill_value for _ in range(data_len)]\n",
    "        for i in range(data_len):\n",
    "            data[i] = to_fill_value\n",
    "        return data\n",
    "    if pd.isna(data[0]):\n",
    "        # find the first non-nan value's position\n",
    "        not_na_pos = 0\n",
    "        for i in range(data_len):\n",
    "            if not pd.isna(data[i]):\n",
    "                not_na_pos = i\n",
    "                break\n",
    "        # fill element before the first non-nan value with median\n",
    "        for i in range(not_na_pos):\n",
    "            data[i] = to_fill_value\n",
    "    # fill element after the first non-nan value\n",
    "    for i in range(1, data_len):\n",
    "        if pd.isna(data[i]):\n",
    "            data[i] = data[i - 1]\n",
    "    return data\n",
    "\n",
    "\n",
    "def forward_fill_pipeline(\n",
    "    df: pd.DataFrame,\n",
    "    default_fill: pd.DataFrame,\n",
    "    demographic_features: list[str],\n",
    "    labtest_features: list[str],\n",
    "    target_features: list[str],\n",
    "):\n",
    "    grouped = df.groupby(\"PatientID\")\n",
    "\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    all_pid = []\n",
    "\n",
    "    for name, group in grouped:\n",
    "        sorted_group = group.sort_values(by=[\"RecordTime\"], ascending=True)\n",
    "        patient_x = []\n",
    "        patient_y = []\n",
    "\n",
    "        for f in [\"Age\"] + labtest_features:\n",
    "            to_fill_value = default_fill[f]\n",
    "            # take median patient as the default to-fill missing value\n",
    "            fill_missing_value(sorted_group[f].values, to_fill_value)\n",
    "\n",
    "        for _, v in sorted_group.iterrows():\n",
    "            y = []\n",
    "            for f in target_features:\n",
    "                y.append(v[f])\n",
    "            patient_y.append([v[\"Outcome\"], v[\"LOS\"]])\n",
    "            x = []\n",
    "            for f in demographic_features + labtest_features:\n",
    "                x.append(v[f])\n",
    "            patient_x.append(x)\n",
    "        all_x.append(patient_x)\n",
    "        all_y.append(patient_y)\n",
    "        all_pid.append(name)\n",
    "    return all_x, all_y, all_pid\n",
    "\n",
    "\n",
    "def filter_outlier(element):\n",
    "    if pd.isna(element):\n",
    "        return 0\n",
    "    elif np.abs(float(element)) > 1e4:\n",
    "        return 0\n",
    "    else:\n",
    "        return element\n",
    "\n",
    "\n",
    "def normalize_dataframe(train_df, val_df, test_df, normalize_features):\n",
    "    # Calculate the quantiles\n",
    "    q_low = train_df[normalize_features].quantile(0.05)\n",
    "    q_high = train_df[normalize_features].quantile(0.95)\n",
    "\n",
    "    # Filter the DataFrame based on the quantiles\n",
    "    filtered_df = train_df[\n",
    "        (train_df[normalize_features] > q_low) & (train_df[normalize_features] < q_high)\n",
    "    ]\n",
    "\n",
    "    # Calculate the mean and standard deviation and median of the filtered data, also the default fill value\n",
    "    train_mean = filtered_df[normalize_features].mean()\n",
    "    train_std = filtered_df[normalize_features].std()\n",
    "    train_median = filtered_df[normalize_features].median()\n",
    "    default_fill: pd.DataFrame = (train_median - train_mean) / (train_std + 1e-12)\n",
    "\n",
    "    # LOS info\n",
    "    los_info = {\n",
    "        \"los_mean\": train_mean[\"LOS\"].item(),\n",
    "        \"los_std\": train_std[\"LOS\"].item(),\n",
    "        \"los_median\": train_median[\"LOS\"].item(),\n",
    "    }\n",
    "\n",
    "    # Calculate large los and threshold (optional, designed for covid-19 benchmark)\n",
    "    los_array = train_df.groupby(\"PatientID\")[\"LOS\"].max().values\n",
    "    los_p95 = np.percentile(los_array, 95)\n",
    "    los_p5 = np.percentile(los_array, 5)\n",
    "    filtered_los = los_array[(los_array >= los_p5) & (los_array <= los_p95)]\n",
    "    los_info.update(\n",
    "        {\"large_los\": los_p95.item(), \"threshold\": filtered_los.mean().item() * 0.5}\n",
    "    )\n",
    "\n",
    "    # Z-score normalize the train, val, and test sets with train_mean and train_std\n",
    "    train_df.loc[:, normalize_features] = (\n",
    "        train_df.loc[:, normalize_features] - train_mean\n",
    "    ) / (train_std + 1e-12)\n",
    "    val_df.loc[:, normalize_features] = (\n",
    "        val_df.loc[:, normalize_features] - train_mean\n",
    "    ) / (train_std + 1e-12)\n",
    "    test_df.loc[:, normalize_features] = (\n",
    "        test_df.loc[:, normalize_features] - train_mean\n",
    "    ) / (train_std + 1e-12)\n",
    "\n",
    "    train_df.loc[:, normalize_features] = train_df.loc[:, normalize_features].map(\n",
    "        filter_outlier\n",
    "    )\n",
    "    val_df.loc[:, normalize_features] = val_df.loc[:, normalize_features].map(\n",
    "        filter_outlier\n",
    "    )\n",
    "    test_df.loc[:, normalize_features] = test_df.loc[:, normalize_features].map(\n",
    "        filter_outlier\n",
    "    )\n",
    "\n",
    "    return train_df, val_df, test_df, default_fill, los_info, train_mean, train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(src_file)\n",
    "\n",
    "basic_records = [\"PatientID\", \"RecordTime\", \"AdmissionTime\", \"DischargeTime\"]\n",
    "target_features = [\"Outcome\", \"LOS\"]\n",
    "demographic_features = [\"Gender\", \"Age\"]\n",
    "labtest_features = list(set(df.columns) - set(basic_records + target_features + demographic_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified split dataset into train, validation and test sets\n",
    "\n",
    "- Also include (Normalization & Outlier Filtering & Imputation) steps.\n",
    "- The train, validation and test sets are saved in the `./processed` folder.\n",
    "- For TJH dataset, use 8:1:1 10-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "num_folds = 10\n",
    "\n",
    "# Group the dataframe by patient ID\n",
    "grouped = df.groupby(\"PatientID\")\n",
    "\n",
    "# Split the patient IDs into train/val/test sets\n",
    "patients = np.array(list(grouped.groups.keys()))\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "for fold, (train_val_index, test_index) in enumerate(\n",
    "    kf.split(patients, df.groupby(\"PatientID\")[\"Outcome\"].first())\n",
    "):\n",
    "    # Get the train/val/test patient IDs for the current fold\n",
    "    train_val_patients, test_patients = patients[train_val_index], patients[test_index]\n",
    "\n",
    "    # Split the train_val_patients into train/val sets\n",
    "    train_patients, val_patients = train_test_split(\n",
    "        train_val_patients,\n",
    "        test_size=1 / (num_folds - 1),\n",
    "        random_state=seed,\n",
    "        stratify=df[df[\"PatientID\"].isin(train_val_patients)]\n",
    "        .groupby(\"PatientID\")[\"Outcome\"]\n",
    "        .first(),\n",
    "    )\n",
    "\n",
    "    # Create train, val, and test dataframes for the current fold\n",
    "    train_df = df[df[\"PatientID\"].isin(train_patients)]\n",
    "    val_df = df[df[\"PatientID\"].isin(val_patients)]\n",
    "    test_df = df[df[\"PatientID\"].isin(test_patients)]\n",
    "\n",
    "    assert len(train_df) + len(val_df) + len(test_df) == len(df)\n",
    "\n",
    "    fold_dir = os.path.join(dst_dir, f\"fold_{fold + 1}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    # Calculate the mean and std of the train set (include age, lab test features, and LOS) on the data in 5% to 95% quantile range\n",
    "    normalize_features = [\"Age\"] + labtest_features + [\"LOS\"]\n",
    "\n",
    "    # Normalize data\n",
    "    train_df, val_df, test_df, default_fill, los_info, train_mean, train_std = normalize_dataframe(train_df, val_df, test_df, normalize_features)\n",
    "\n",
    "    # Drop rows if all features are recorded NaN\n",
    "    train_df = train_df.dropna(axis=0, how=\"all\", subset=normalize_features)\n",
    "    val_df = val_df.dropna(axis=0, how=\"all\", subset=normalize_features)\n",
    "    test_df = test_df.dropna(axis=0, how=\"all\", subset=normalize_features)\n",
    "\n",
    "    # Forward Imputation after grouped by PatientID\n",
    "    # Notice: if a patient has never done certain lab test, the imputed value will be the median value calculated from train set\n",
    "    train_x, train_y, train_pid = forward_fill_pipeline(\n",
    "        train_df, default_fill, demographic_features, labtest_features, target_features\n",
    "    )\n",
    "    val_x, val_y, val_pid = forward_fill_pipeline(\n",
    "        val_df, default_fill, demographic_features, labtest_features, target_features\n",
    "    )\n",
    "    test_x, test_y, test_pid = forward_fill_pipeline(\n",
    "        test_df, default_fill, demographic_features, labtest_features, target_features\n",
    "    )\n",
    "\n",
    "    # Save the imputed dataset to pickle file\n",
    "    pd.to_pickle(train_x, os.path.join(fold_dir, \"train_x.pkl\"))\n",
    "    pd.to_pickle(train_y, os.path.join(fold_dir, \"train_y.pkl\"))\n",
    "    pd.to_pickle(train_pid, os.path.join(fold_dir, \"train_pid.pkl\"))\n",
    "    pd.to_pickle(val_x, os.path.join(fold_dir, \"val_x.pkl\"))\n",
    "    pd.to_pickle(val_y, os.path.join(fold_dir, \"val_y.pkl\"))\n",
    "    pd.to_pickle(val_pid, os.path.join(fold_dir, \"val_pid.pkl\"))\n",
    "    pd.to_pickle(test_x, os.path.join(fold_dir, \"test_x.pkl\"))\n",
    "    pd.to_pickle(test_y, os.path.join(fold_dir, \"test_y.pkl\"))\n",
    "    pd.to_pickle(test_pid, os.path.join(fold_dir, \"test_pid.pkl\"))\n",
    "    pd.to_pickle(los_info, os.path.join(fold_dir, \"los_info.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
